{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139808, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>spans</th>\n",
       "      <th>country_predicted</th>\n",
       "      <th>country_conf</th>\n",
       "      <th>admin1</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>country_code3</th>\n",
       "      <th>geonameid</th>\n",
       "      <th>place_name</th>\n",
       "      <th>feature_class</th>\n",
       "      <th>feature_code</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>[{'start': 162, 'end': 170}]</td>\n",
       "      <td>CHN</td>\n",
       "      <td>0.963741</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>31.22222</td>\n",
       "      <td>121.45806</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1796236.0</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLA</td>\n",
       "      <td>4963912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aeromonas</td>\n",
       "      <td>[{'start': 1153, 'end': 1162}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4729030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>[{'start': 57, 'end': 65}]</td>\n",
       "      <td>THA</td>\n",
       "      <td>0.948191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.50000</td>\n",
       "      <td>101.00000</td>\n",
       "      <td>THA</td>\n",
       "      <td>1605651.0</td>\n",
       "      <td>Kingdom of Thailand</td>\n",
       "      <td>A</td>\n",
       "      <td>PCLI</td>\n",
       "      <td>4695222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangkok</td>\n",
       "      <td>[{'start': 302, 'end': 309}]</td>\n",
       "      <td>THA</td>\n",
       "      <td>0.961249</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>13.75398</td>\n",
       "      <td>100.50144</td>\n",
       "      <td>THA</td>\n",
       "      <td>1609350.0</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLC</td>\n",
       "      <td>4695222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Israel</td>\n",
       "      <td>[{'start': 285, 'end': 291}]</td>\n",
       "      <td>ISR</td>\n",
       "      <td>0.948191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.50000</td>\n",
       "      <td>34.75000</td>\n",
       "      <td>ISR</td>\n",
       "      <td>294640.0</td>\n",
       "      <td>State of Israel</td>\n",
       "      <td>A</td>\n",
       "      <td>PCLI</td>\n",
       "      <td>4762134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                           spans country_predicted  country_conf  \\\n",
       "0   Shanghai    [{'start': 162, 'end': 170}]               CHN      0.963741   \n",
       "5  Aeromonas  [{'start': 1153, 'end': 1162}]               NaN      0.000000   \n",
       "6   Thailand      [{'start': 57, 'end': 65}]               THA      0.948191   \n",
       "7    Bangkok    [{'start': 302, 'end': 309}]               THA      0.961249   \n",
       "9     Israel    [{'start': 285, 'end': 291}]               ISR      0.948191   \n",
       "\n",
       "     admin1       lat        lon country_code3  geonameid  \\\n",
       "0  Shanghai  31.22222  121.45806           CHN  1796236.0   \n",
       "5       NaN       NaN        NaN           NaN        NaN   \n",
       "6       NaN  15.50000  101.00000           THA  1605651.0   \n",
       "7   Bangkok  13.75398  100.50144           THA  1609350.0   \n",
       "9       NaN  31.50000   34.75000           ISR   294640.0   \n",
       "\n",
       "            place_name feature_class feature_code   doc_id  \n",
       "0             Shanghai             P         PPLA  4963912  \n",
       "5                  NaN           NaN          NaN  4729030  \n",
       "6  Kingdom of Thailand             A         PCLI  4695222  \n",
       "7              Bangkok             P         PPLC  4695222  \n",
       "9      State of Israel             A         PCLI  4762134  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we load our location data and drop any duplicated places within documents\n",
    "import pandas as pd\n",
    "import shapely.vectorized\n",
    "from global_land_mask import globe\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Load location data \n",
    "places = pd.read_csv('data/clean_places.csv')                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "places = places.drop_duplicates([\"doc_id\",\"geonameid\"]) \n",
    "print(places.shape)\n",
    "places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>0 - relevant</th>\n",
       "      <th>2 - 1.02. Changes in temperature</th>\n",
       "      <th>2 - 1.03. Seasonal change</th>\n",
       "      <th>2 - 1.04. Changes in precipitation</th>\n",
       "      <th>2 - 1.06. Climate change (unspecified)</th>\n",
       "      <th>2 - 1.07. Other meteorological variables</th>\n",
       "      <th>3 - 2.01. Food security</th>\n",
       "      <th>...</th>\n",
       "      <th>4 - 2.04. Extreme event attribution</th>\n",
       "      <th>4 - 2.05. Scenarios</th>\n",
       "      <th>5 - 4.01. Floods and drought</th>\n",
       "      <th>5 - 4.02. Heatwaves</th>\n",
       "      <th>5 - 4.03. Wildfires</th>\n",
       "      <th>5 - 4.04. Other extreme events</th>\n",
       "      <th>5 - 4.05. Extreme cold</th>\n",
       "      <th>6 - 5.01. Pollution</th>\n",
       "      <th>6 - 5.03. Reduced agricultural &amp; aquaculture productivity</th>\n",
       "      <th>6 - 5.04. Reduced labour and physical capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20881</td>\n",
       "      <td>Climate change will result in more intense, mo...</td>\n",
       "      <td>Projections of heat waves with high impact on ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27158</td>\n",
       "      <td>Temperature, a key climate change indicator, i...</td>\n",
       "      <td>Apparent Temperature and Cause-Specific Mortal...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30042</td>\n",
       "      <td>Climate change has led to significant rise of ...</td>\n",
       "      <td>Climate Change and the Emergent Epidemic of CK...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30687</td>\n",
       "      <td>BACKGROUND: The periods of fetal and child dev...</td>\n",
       "      <td>Children are likely to suffer most from our fo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35987</td>\n",
       "      <td>Background Extreme precipitation events are in...</td>\n",
       "      <td>Autochthonous Chikungunya Transmission and Ext...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content  \\\n",
       "0  20881  Climate change will result in more intense, mo...   \n",
       "1  27158  Temperature, a key climate change indicator, i...   \n",
       "2  30042  Climate change has led to significant rise of ...   \n",
       "3  30687  BACKGROUND: The periods of fetal and child dev...   \n",
       "4  35987  Background Extreme precipitation events are in...   \n",
       "\n",
       "                                               title  0 - relevant  \\\n",
       "0  Projections of heat waves with high impact on ...           1.0   \n",
       "1  Apparent Temperature and Cause-Specific Mortal...           1.0   \n",
       "2  Climate Change and the Emergent Epidemic of CK...           1.0   \n",
       "3  Children are likely to suffer most from our fo...           1.0   \n",
       "4  Autochthonous Chikungunya Transmission and Ext...           1.0   \n",
       "\n",
       "   2 - 1.02. Changes in temperature  2 - 1.03. Seasonal change  \\\n",
       "0                               1.0                        0.0   \n",
       "1                               1.0                        0.0   \n",
       "2                               1.0                        0.0   \n",
       "3                               1.0                        0.0   \n",
       "4                               0.0                        0.0   \n",
       "\n",
       "   2 - 1.04. Changes in precipitation  2 - 1.06. Climate change (unspecified)  \\\n",
       "0                                 0.0                                     0.0   \n",
       "1                                 0.0                                     0.0   \n",
       "2                                 0.0                                     0.0   \n",
       "3                                 0.0                                     0.0   \n",
       "4                                 1.0                                     0.0   \n",
       "\n",
       "   2 - 1.07. Other meteorological variables  3 - 2.01. Food security  ...  \\\n",
       "0                                       1.0                      0.0  ...   \n",
       "1                                       0.0                      0.0  ...   \n",
       "2                                       0.0                      0.0  ...   \n",
       "3                                       0.0                      1.0  ...   \n",
       "4                                       0.0                      0.0  ...   \n",
       "\n",
       "   4 - 2.04. Extreme event attribution  4 - 2.05. Scenarios  \\\n",
       "0                                  1.0                  1.0   \n",
       "1                                  0.0                  0.0   \n",
       "2                                  1.0                  0.0   \n",
       "3                                  0.0                  0.0   \n",
       "4                                  0.0                  0.0   \n",
       "\n",
       "   5 - 4.01. Floods and drought  5 - 4.02. Heatwaves  5 - 4.03. Wildfires  \\\n",
       "0                           0.0                  1.0                  0.0   \n",
       "1                           0.0                  0.0                  0.0   \n",
       "2                           0.0                  1.0                  0.0   \n",
       "3                           0.0                  0.0                  0.0   \n",
       "4                           0.0                  0.0                  0.0   \n",
       "\n",
       "   5 - 4.04. Other extreme events  5 - 4.05. Extreme cold  \\\n",
       "0                             0.0                     0.0   \n",
       "1                             0.0                     0.0   \n",
       "2                             0.0                     0.0   \n",
       "3                             0.0                     0.0   \n",
       "4                             0.0                     0.0   \n",
       "\n",
       "   6 - 5.01. Pollution  \\\n",
       "0                  0.0   \n",
       "1                  0.0   \n",
       "2                  0.0   \n",
       "3                  0.0   \n",
       "4                  0.0   \n",
       "\n",
       "   6 - 5.03. Reduced agricultural & aquaculture productivity  \\\n",
       "0                                                0.0           \n",
       "1                                                0.0           \n",
       "2                                                0.0           \n",
       "3                                                0.0           \n",
       "4                                                0.0           \n",
       "\n",
       "   6 - 5.04. Reduced labour and physical capacity  \n",
       "0                                             1.0  \n",
       "1                                             0.0  \n",
       "2                                             0.0  \n",
       "3                                             0.0  \n",
       "4                                             0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_feather(\"data/included_studies.feather\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-88.75</td>\n",
       "      <td>-178.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-88.75</td>\n",
       "      <td>-176.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-88.75</td>\n",
       "      <td>-173.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-88.75</td>\n",
       "      <td>-171.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-88.75</td>\n",
       "      <td>-168.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LAT     LON\n",
       "0 -88.75 -178.75\n",
       "1 -88.75 -176.25\n",
       "2 -88.75 -173.75\n",
       "3 -88.75 -171.25\n",
       "4 -88.75 -168.75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def generate_grid_df(degrees):\n",
    "    '''\n",
    "    Generate a dataframe with a grid of of cells degrees x degrees\n",
    "    '''\n",
    "    LON = np.linspace(-180+degrees*0.5,180-degrees*0.5,int(360/degrees))\n",
    "    LAT = np.linspace(-90+degrees*0.5,90-degrees*0.5,int(180/degrees))\n",
    "    lon_df, lat_df = np.meshgrid(LON,LAT)\n",
    "\n",
    "    return pd.DataFrame({\"LAT\": lat_df.ravel(), \"LON\": lon_df.ravel()})\n",
    "    \n",
    "grid_df = generate_grid_df(2.5)\n",
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_375273/1055013544.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas\n"
     ]
    }
   ],
   "source": [
    "import cartopy.io.shapereader as shpreader\n",
    "import geopandas\n",
    "# First we define a list of the shapefile definitions we want\n",
    "shpfiles = [\n",
    "    dict(resolution='50m', category='cultural', name='admin_0_countries'),\n",
    "    dict(resolution='10m', category='cultural', name='admin_1_states_provinces'),\n",
    "    dict(resolution='10m', category='physical', name='geography_regions_polys'),\n",
    "    dict(resolution='10m', category='physical', name='geography_marine_polys')\n",
    "    #\"data/gadm36_1.shp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading shapefile {'resolution': '50m', 'category': 'cultural', 'name': 'admin_0_countries'}\n",
      "reading shapefile {'resolution': '10m', 'category': 'cultural', 'name': 'admin_1_states_provinces'}\n",
      "reading shapefile {'resolution': '10m', 'category': 'physical', 'name': 'geography_regions_polys'}\n",
      "reading shapefile {'resolution': '10m', 'category': 'physical', 'name': 'geography_marine_polys'}\n"
     ]
    }
   ],
   "source": [
    "# We'll start an empty dataframe to store our shapefile-grid matches\n",
    "shp_grid_df = pd.DataFrame()\n",
    "\n",
    "# Now we download the shapefiles and combine into one large geopandas dataframe\n",
    "shp_df  = None\n",
    "for shpfilename in shpfiles:\n",
    "    print(f\"reading shapefile {shpfilename}\")\n",
    "    if shp_df is None:\n",
    "        shp_df = geopandas.read_file(shpreader.natural_earth(**shpfilename))\n",
    "    else:\n",
    "        shp_df = shp_df.merge(geopandas.read_file(shpreader.natural_earth(**shpfilename)),how=\"outer\")\n",
    "    time.sleep(10) # Wait a bit before downloading the next shapefile so we do not make too many requests too quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shpfile_id</th>\n",
       "      <th>grid_df_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shpfile_id  grid_df_id\n",
       "0           0        4114\n",
       "1           0        3971\n",
       "2           0        4115\n",
       "3           0        3972\n",
       "4           0        4116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "degrees = 2.5\n",
    "\n",
    "# We are going to store our shapefile-gridcell index matches here\n",
    "shp_grid = []\n",
    "\n",
    "# This is the grid we will work with\n",
    "yv, xv = np.meshgrid(grid_df.LAT.unique(), grid_df.LON.unique())\n",
    "for i, place in shp_df.iterrows(): # Now we go through all the shapes\n",
    "    # show which gridcell centers are contained inside the shape\n",
    "    # ignore the warning caused by shapely using an old version of numpy\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        inplace = shapely.vectorized.contains(place.geometry, xv, yv)\n",
    "    idx = np.argwhere(inplace)\n",
    "    # Get the number of cells contained in the shape\n",
    "    number_cells = idx.size/2\n",
    "    if number_cells == 0:\n",
    "        # If we have no cell centers in the shape, get the shape center and the cell which contains it\n",
    "        c = place.geometry.centroid\n",
    "        lon = c.x//degrees*degrees+degrees*0.5\n",
    "        lat = c.y//degrees*degrees+degrees*0.5\n",
    "        da_df = grid_df[(grid_df['LON']==lon) & (grid_df['LAT']==lat)]\n",
    "        shp_grid.append({\"shpfile_id\": i, \"grid_df_id\": da_df.index[0]})\n",
    "    else:\n",
    "        for point in idx:\n",
    "            lon = grid_df.LON.unique()[point[0]]\n",
    "            lat = grid_df.LAT.unique()[point[1]]\n",
    "            da_df = grid_df[(grid_df['LON']==lon) & (grid_df['LAT']==lat)]\n",
    "            shp_grid.append({\"shpfile_id\": i, \"grid_df_id\": da_df.index[0]}) \n",
    "\n",
    "shp_grid_df = pd.DataFrame.from_dict(shp_grid)\n",
    "    \n",
    "\n",
    "shp_grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping = {\n",
    "    \"ADM1\": [\"Admin-1 scale rank\", \"Admin-1 aggregation\", \"Admin-1 minor island\"],\n",
    "    \"PCLI\": [\"Admin-0 country\"],\n",
    "    'MTS': ['Range/mtn'],\n",
    "    'PLAT': ['Plateau'],\n",
    "    'PLN': ['Plain'],\n",
    "    'DSRT': ['Desert'],\n",
    "    'OCN': ['ocean'],\n",
    "    'SEA': ['sea', 'bay'],\n",
    "    'GULF': ['gulf', 'bay'],\n",
    "    'BAY': ['gulf', 'bay'],\n",
    "    'CHN': ['channel'],\n",
    "    'BSNU': ['basin']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add all the other codes we don't cover with blank shapefile classes\n",
    "for fcode in places.feature_code.unique():\n",
    "    if fcode not in feature_mapping:\n",
    "        feature_mapping[fcode] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375273/2263670162.py:43: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  places[\"place_name\"] = places[\"place_name\"].str.lower().str.replace(\"mts.\",\"mountains\")\n",
      "/tmp/ipykernel_375273/2263670162.py:44: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  shp_df[\"name\"] = shp_df[\"name\"].str.lower().str.replace(\"mts.\",\"mountains\")\n"
     ]
    }
   ],
   "source": [
    "# To help match, we will rename some shapes so that they are the same as the name in our database\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Altay\", case=False)),\"name\"] = \"Altay\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Appalach\", case=False)),\"name\"] = \"Appalachian Mountains\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"cant\", case=False)),\"name\"] = \"Cordillera Cantábrica\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Dabie\", case=False)),\"name\"] = \"Dabie Shan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"EASTERN GHATS\", case=False)),\"name\"] = \"Eastern Ghāts\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"WESTERN GHATS\", case=False)),\"name\"] = \"Western Ghāts\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"kunlun\", case=False)),\"name\"] = \"Kalakunlun Shan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"LEN MOUNTAIN\", case=False)),\"name\"] = \"Kölen\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Taihang Mts.\", case=False)),\"name\"] = \"Taihang Shan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Tatra Mts.\", case=False)),\"name\"] = \"Tatry\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"TIAN SHAN\", case=False)),\"name\"] = \"Tien Shan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"andes\", case=False)),\"name\"] = \"Andes Mountains\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"HINDU KUSH\", case=False)),\"name\"] = \"Hindū Kush\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Marrah Mts\", case=False)),\"name\"] = \"Jabal Marrah\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"Lebanon\", case=False)),\"name\"] = \"Mount Lebanon\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Range/mtn\") & (shp_df[\"name\"].str.contains(\"KARAKORAM RA\", case=False)),\"name\"] = \"Karakorum Shan\"\n",
    "\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Desert\") & (shp_df[\"name\"].str.contains(\"Negev\", case=False)), \"name\"] = \"Negev\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Desert\") & (shp_df[\"name\"].str.contains(\"Atacama\", case=False)), \"name\"] = \"Atacama Desert\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Desert\") & (shp_df[\"name\"].str.contains(\"CHIHUAHUAN DESERT\", case=False)), \"name\"] = \"Chihuahua Desert\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Desert\") & (shp_df[\"name\"].str.contains(\"Lut desert\", case=False)), \"name\"] = \"God-e Lut\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Desert\") & (shp_df[\"name\"].str.contains(\"TAKLIMAKAN DESERT\", case=False)), \"name\"] = \"Takla Makan Desert\"\n",
    "\n",
    "shp_df.loc[\n",
    "    (shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"cumberland\", case=False)),[\"name\",\"featurecla\"]\n",
    "] = [\"Cumberland Plateau\", \"Plain\"]\n",
    "shp_df.loc[\n",
    "    (shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"colorado\", case=False)),[\"name\",\"featurecla\"]\n",
    "] = [\"San Francisco Plateau\", \"Plain\"]\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plain\") & (shp_df[\"name\"].str.contains(\"gange\", case=False)),\"name\"] = \"Gangetic Plain\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plain\") & (shp_df[\"name\"].str.contains(\"north china\", case=False)),\"name\"] = \"Huanghuai Pingyuan\"\n",
    "\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"mongol\", case=False)), \"name\"] = \"Nei Mongol Gaoyuan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"deccan\", case=False)), \"name\"] = \"Deccan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"chota\", case=False)), \"name\"] = \"Chota Nāgpur Plateau\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"loess\", case=False)), \"name\"] = \"Huangtu Gaoyuan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"khorat\", case=False)), \"name\"] = \"Khorat Plateau\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"tibet\", case=False)), \"name\"] = \"Qing Zang Gaoyuan\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"polar\", case=False)), \"name\"] = \"South Polar Plateau\"\n",
    "shp_df.loc[(shp_df[\"featurecla\"]==\"Plateau\") & (shp_df[\"name\"].str.contains(\"YUNGUI\", case=False)), \"name\"] = \"Yungui Gaoyuan\"\n",
    "\n",
    "places[\"place_name\"] = places[\"place_name\"].str.lower().str.replace(\"mts.\",\"mountains\") \n",
    "shp_df[\"name\"] = shp_df[\"name\"].str.lower().str.replace(\"mts.\",\"mountains\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM1\n",
      "PCLI\n"
     ]
    }
   ],
   "source": [
    "# We'll start off with an empty dataframe\n",
    "shp_df_matches = pd.DataFrame()\n",
    "\n",
    "# And define the grid box size\n",
    "degrees = 2.5\n",
    "\n",
    "# Now we want to look through the feature type dictionary we had before\n",
    "for place_key, shpfile_keys in feature_mapping.items():\n",
    "    \n",
    "    print(place_key)\n",
    "    \n",
    "    # We can get all the places and all the shapes\n",
    "    feature_places = places.loc[places[\"feature_code\"]==place_key]\n",
    "    feature_shapes = shp_df.loc[shp_df[\"featurecla\"].isin(shpfile_keys)]\n",
    "    \n",
    "    # And loop through the places\n",
    "    for place, group in feature_places.groupby(\"place_name\"):\n",
    "        # If we don't have a shapefile feature type we just take the grid cell containing the point\n",
    "        if not shpfile_keys:\n",
    "            shp_id = None # we set shp_id to None, round the coordinates, and take the gridcell which has these coordinates\n",
    "            lon = group.lon.values[0]//degrees*degrees+degrees*0.5\n",
    "            lat = group.lat.values[0]//degrees*degrees+degrees*0.5\n",
    "            grid_df_ids = grid_df[(grid_df['LON']==lon) & (grid_df['LAT']==lat)].index\n",
    "\n",
    "        else:\n",
    "            # Otherwise, we are going to try to find the places which match\n",
    "            place_shapes = pd.DataFrame()\n",
    "            # First we will try by geoname ids, if we have these\n",
    "            if feature_shapes[\"gn_id\"].sum() > 0:\n",
    "                place_shapes = feature_shapes.loc[\n",
    "                    (feature_shapes[\"gn_id\"]==group.geonameid.values[0])\n",
    "                ]\n",
    "            # Then we try by country code, if we are dealing with a country\n",
    "            if place_shapes.shape[0]==0 and place_key==\"PCLI\":\n",
    "                place_shapes = feature_shapes.loc[\n",
    "                    feature_shapes[\"ADM0_A3\"] == group.country_predicted.values[0]\n",
    "                ]\n",
    "            # Then we try by name\n",
    "            if place_shapes.shape[0]==0:       \n",
    "                place_shapes = feature_shapes.loc[\n",
    "                    (feature_shapes.name == place)\n",
    "                ]    \n",
    "\n",
    "            if place_shapes.shape[0]==0:\n",
    "                continue # These are the places we cannot match, we need to develop other strategies for these, e.g. using other shapefile libraries\n",
    "    #            if key==\"ADM1\": # can also try using gadm data, which we need to download separately\n",
    "    #                 place_shapes = adm1shps_alt[\n",
    "    #                     (adm1shps_alt[\"NAME_1\"]==group.place_name.values[0]) |\n",
    "    #                     (adm1shps_alt[\"VARNAME_1\"].str.contains(group.place_name.values[0]))\n",
    "    #                 ]\n",
    "            else:\n",
    "                shp_id = place_shapes.index[0]\n",
    "                grid_df_ids = shp_grid_df.loc[\n",
    "                    (shp_grid_df[\"shpfile_id\"]==shp_id),\n",
    "                    \"grid_df_id\"\n",
    "                ]\n",
    "        # For each document with this place, we add a row for each grid cell index matching the place\n",
    "        for did in group.doc_id.unique():\n",
    "            shp_df_matches = pd.concat([\n",
    "                shp_df_matches,\n",
    "                pd.DataFrame.from_dict({\n",
    "                    \"grid_df_id\": grid_df_ids,\n",
    "                    \"doc_id\": [did] * len(grid_df_ids),\n",
    "                    \"shp_id\": [shp_id] * len(grid_df_ids),\n",
    "                    \"place\": place\n",
    "                })\n",
    "            ])\n",
    "            \n",
    "            \n",
    "print(shp_df_matches.shape)\n",
    "shp_df_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_df_matches.to_csv(\"data/shp_df_matches.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "rds = rioxarray.open_rasterio(\"data/gpw_v4_population_count_rev11_2020_1_deg.asc\",\n",
    ")\n",
    "\n",
    "rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "rds.name = \"population\"\n",
    "pop_df = rds.to_dataframe().reset_index()\n",
    "\n",
    "pop_df['LAT'] = pop_df['y']//2.5*2.5+1.25\n",
    "pop_df['LON'] = pop_df['x']//2.5*2.5+1.25\n",
    "pop_df.loc[pop_df['population']==-9999,\"population\"]=np.NaN\n",
    "pop_df = pop_df.groupby([\"LAT\",\"LON\"])['population'].sum().reset_index()\n",
    "pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def area_cell(lat, lon, degrees): \n",
    "    # calculate the area of a gridcell given the center lat and lon and the size in degrees\n",
    "    if lon <0:\n",
    "        lon+=360\n",
    "    R = 6371\n",
    "    f0 = math.radians(lat-degrees*0.5)\n",
    "    f1 = math.radians(lat+degrees*0.5)\n",
    "    l0 = math.radians(lon-degrees*0.5)\n",
    "    l1 = math.radians(lon+degrees*0.5)\n",
    "\n",
    "    return (math.sin(f1)-math.sin(f0)) * (l1 - l0) * R**2\n",
    "\n",
    "\n",
    "grid_df['area'] = grid_df.apply(lambda x: area_cell(x['LAT'], x['LON'], 2.5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,2.5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_masks = []\n",
    "n = 5\n",
    "land_array = np.empty((grid_df.shape[0], n*n))\n",
    "i = 0\n",
    "for x in np.linspace(0,2.5,n):\n",
    "    for y in np.linspace(0,2.5,n):\n",
    "        land_array[:,i] = globe.is_land(grid_df.LAT+y-1.25, grid_df.LON+x-1.25)\n",
    "        i+=1\n",
    "        \n",
    "land_mask = land_array.sum(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['is_land'] = land_mask\n",
    "grid_df.loc[grid_df['LAT']<-60,'is_land'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.is_land.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc_data = pd.read_csv(\"data/2_merged_da_data.csv\")\n",
    "grid_df = grid_df.merge(\n",
    "    ncc_data[[\"LAT\",\"LON\",\"updated_precip\",\"updated_temp\"]].rename(columns={\"updated_precip\": \"precip_da\", \"updated_temp\": \"temp_da\"})\n",
    ").merge(pop_df)\n",
    "grid_df.reset_index().to_csv(\"data/grid_df.csv\", index=False)\n",
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
